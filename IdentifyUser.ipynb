{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request,json\n",
    "import datetime\n",
    "import torch\n",
    "import string\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifyUser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.target = \"Be Authentic. Be Yourself. Be Typing.\"\n",
    "        self.pad_length = 45\n",
    "        self.num_epochs = 4\n",
    "        self.encodeChars()\n",
    "        self.validateUsers()\n",
    "        self.buildUserIndexMaps()\n",
    "        self.partitionData()\n",
    "        vec_length,output_size = len(self.charMap)+1,len(self.valid_users)\n",
    "        self.keynet = KeyStrokeNet(self.pad_length,vec_length,output_size)\n",
    "        self.train_net(self.num_epochs)\n",
    "     \n",
    "    def encodeChars(self):\n",
    "        self.charMap = {}\n",
    "        keyboard_chars = string.ascii_letters + string.digits + string.punctuation\n",
    "        for i in range(len(keyboard_chars)):\n",
    "            key = keyboard_chars[i]\n",
    "            self.charMap[key] = i\n",
    "        self.charMap['[backspace]'] = len(keyboard_chars)\n",
    "        self.charMap[' '] = len(keyboard_chars) + 1\n",
    "            \n",
    "            \n",
    "    def buildUserIndexMaps(self):\n",
    "        self.userMap,self.indexMap = {},{}\n",
    "        for i in range(len(self.valid_users)):\n",
    "            user = self.valid_users[i]\n",
    "            self.userMap[user] = i\n",
    "            self.indexMap[i] = user\n",
    "       \n",
    "    def isValid(self,type_dict):\n",
    "        s = \"\"\n",
    "        for one_type in type_dict:\n",
    "            char = one_type['character']\n",
    "            if char == '[backspace]':\n",
    "                if len(s) > 0:\n",
    "                    s = s[:len(s)-1]\n",
    "            else:\n",
    "                s = s + char\n",
    "        return (s == self.target)\n",
    "     \n",
    "    def validateUsers(self):\n",
    "        url_str = \"user_4a438fdede4e11e9b986acde48001122.json\"\n",
    "        prefix = \"https://challenges.unify.id/v1/mle/\"\n",
    "        target = \"Be Authentic. Be Yourself. Be Typing.\"\n",
    "        self.valid_users, self.invalid_users = [],[]\n",
    "        self.examples,self.labels = [],[]\n",
    "\n",
    "        while url_str != None:\n",
    "\n",
    "            with urllib.request.urlopen(prefix + url_str) as url:\n",
    "\n",
    "                agg_user_data = json.loads(url.read().decode())\n",
    "                user_data = agg_user_data['user_data']\n",
    "                user_label = agg_user_data['user_label']\n",
    "                next_user = agg_user_data['next']\n",
    "                url_str = next_user\n",
    "\n",
    "                num_valid_strings = 0\n",
    "\n",
    "                for type_dict in user_data:\n",
    "                    if self.isValid(type_dict):\n",
    "                        num_valid_strings += 1\n",
    "\n",
    "                valid = False\n",
    "                if num_valid_strings >= 300:\n",
    "                    self.valid_users.append(user_label)\n",
    "                    self.examples.extend(user_data)\n",
    "                    self.labels.extend([user_label]*len(user_data))\n",
    "                else:\n",
    "                    self.invalid_users.append(user_label)\n",
    "\n",
    "    \n",
    "    def train_net(self,epochs):\n",
    "\n",
    "        ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.keynet.parameters(), lr=4e-3)\n",
    "        for e in range(0, epochs):\n",
    "            for batch_idx, batch in enumerate(self.train_dataloader):\n",
    "                row = batch['row']\n",
    "                label = batch['label'].squeeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                label_probs = self.keynet.forward(row)\n",
    "                \n",
    "                loss = ce_loss(label_probs,label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                pred_label = torch.argmax(label_probs,dim=1)\n",
    "                batch_accuracy = torch.sum(pred_label == label)\n",
    "\n",
    "            train_accuracy = batch_accuracy/len(self.train_dataloader.dataset)\n",
    "\n",
    "            with torch.no_grad():    \n",
    "                for batch_idx, batch in enumerate(self.val_dataloader):\n",
    "                    row = batch['row']\n",
    "                    label = batch['label']\n",
    "                    label_probs = self.keynet.forward(row)\n",
    "                    pred_label = torch.argmax(label_probs,dim=1)\n",
    "                    test_accuracy = 100*torch.sum(pred_label == label)/len(label)\n",
    "\n",
    "            print('Epoch: {} \\tTrain_Accuracy: {:.5f}'.format(\n",
    "                  e, train_accuracy))\n",
    "            print('Epoch: {} \\tTest_Accuracy: {:.5f}'.format(\n",
    "                  e, test_accuracy))\n",
    "            \n",
    "    def partitionData(self):\n",
    "        indices = np.arange(len(self.labels))\n",
    "        np.random.shuffle(indices)\n",
    "        test_indices = indices[:len(indices)//4]\n",
    "        train_indices = indices[len(indices)//4:]\n",
    "        val_data = np.take(self.examples,test_indices)\n",
    "        train_data = np.take(self.examples,train_indices)\n",
    "        val_labels = np.take(self.labels,test_indices)\n",
    "        train_labels = np.take(self.labels,train_indices)\n",
    "        \n",
    "        train_dataset = KeyStrokeDataset(train_data,train_labels,True,\n",
    "                                            self.charMap,self.userMap,self.pad_length)\n",
    "        val_dataset = KeyStrokeDataset(val_data,val_labels,True,\n",
    "                                            self.charMap,self.userMap,self.pad_length)\n",
    "        \n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=150)\n",
    "        self.val_dataloader = DataLoader(val_dataset,batch_size=len(val_dataset))\n",
    "    \n",
    "    def predict(self,examples):\n",
    "        test_dataset = KeyStrokeDataset(examples,None,False,self.charMap,\n",
    "                                        self.userMap,self.pad_length)\n",
    "        test_dataloader(test_dataset,batch_size=len(test_dataset))\n",
    "        for batch_idx, batch in enumerate(self.val_dataloader):\n",
    "            row = batch['row']\n",
    "            label_probs = keynet.forward(row)\n",
    "            pred_label = torch.argmax(label_probs,dim=1)\n",
    "        \n",
    "        return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyStrokeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data,labels,hasLabels,charMap,userMap,pad_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.charMap = charMap\n",
    "        self.userMap = userMap\n",
    "        self.hasLabels = hasLabels\n",
    "        self.pad_length = pad_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        type_arr = self.data[index]\n",
    "        row = []\n",
    "        for i in range(0,len(type_arr)):\n",
    "            one_type = type_arr[i]\n",
    "            array = np.zeros(len(self.charMap)+1)\n",
    "            index = self.charMap[one_type['character']]\n",
    "            array[index+1] = 1\n",
    "            curr = pd.to_datetime(one_type['typed_at'])\n",
    "            if i == 0:\n",
    "                time_diff = 0\n",
    "            else:\n",
    "                time_diff = (curr - last).total_seconds()  \n",
    "            last = curr\n",
    "            array[0] = time_diff*1000\n",
    "            row.append(array)\n",
    "        row = torch.Tensor(row)\n",
    "        \n",
    "        if len(row) > self.pad_length:\n",
    "            pad_tensor = row[:pad_length]\n",
    "        else:\n",
    "            pad_tensor = torch.zeros(self.pad_length,len(row[0]))\n",
    "            pad_tensor[:len(row)] = row \n",
    "        \n",
    "        pad_tensor = pad_tensor.unsqueeze(0)\n",
    "        \n",
    "        if self.hasLabels:\n",
    "            label = torch.Tensor([self.userMap[self.labels[index]]]).type(torch.LongTensor)\n",
    "        else:\n",
    "            label = None\n",
    "            \n",
    "        sample = {'row':pad_tensor, 'label':label}\n",
    "        \n",
    "        return sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyStrokeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, pad_length,vec_length,output_size):\n",
    "        super(KeyStrokeNet, self).__init__()\n",
    "        self.width = vec_length\n",
    "        self.height = pad_length\n",
    "        num_convs = 3\n",
    "        self.pooling = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.lrelu = torch.nn.LeakyReLU()\n",
    "        in_chans,out_chans = 1,3\n",
    "        conv_op = []\n",
    "        for i in range(num_convs):\n",
    "            conv_op.append(torch.nn.Conv2d(in_chans,out_chans,kernel_size = 5, padding = 2))\n",
    "            in_chans,out_chans = out_chans,out_chans*2\n",
    "            conv_op.append(self.pooling)\n",
    "            self.width,self.height = self.width//2,self.height//2\n",
    "            conv_op.append(self.lrelu)\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*conv_op)\n",
    "            \n",
    "        self.in_chans = in_chans\n",
    "        self.fc = torch.nn.Linear(self.width*self.height*in_chans,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv_layers(x)\n",
    "        out1 = out1.reshape(-1,self.in_chans*self.width*self.height)\n",
    "        out2 = self.fc(out1)\n",
    "        out2 = self.lrelu(out2)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idu = IdentifyUser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
